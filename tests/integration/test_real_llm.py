"""
Integration tests for real LLM APIs and local models.

Tests cover:
- OpenAI API integration (rate limits, error handling)
- Local model integration (Ollama/llama.cpp)
- Anthropic Claude API integration
- Latency distribution measurements
- Moral filter validation with toxic inputs
"""

import time
from unittest.mock import Mock

import numpy as np
import pytest

from mlsdm.core.llm_wrapper import LLMWrapper

# ============================================================================
# Mock Embedding Function
# ============================================================================

def mock_embedding_fn(text: str) -> np.ndarray:
    """Mock embedding function that creates deterministic embeddings."""
    seed = sum(ord(c) for c in text) % (2**32)
    np.random.seed(seed)
    vec = np.random.randn(384).astype(np.float32)
    norm = np.linalg.norm(vec)
    return vec / (norm + 1e-9)


# ============================================================================
# OpenAI API Tests
# ============================================================================

class TestOpenAIIntegration:
    """Test integration with OpenAI API."""

    @pytest.fixture
    def openai_mock(self) -> Mock:
        """Create mock OpenAI client."""
        mock_client = Mock()
        mock_response = Mock()
        mock_response.choices = [Mock(message=Mock(content="Test response"))]
        mock_client.chat.completions.create.return_value = mock_response
        return mock_client

    def test_openai_basic_call(self, openai_mock: Mock) -> None:
        """Test basic OpenAI API call."""

        def llm_generate(prompt: str, max_tokens: int = 100) -> str:
            response = openai_mock.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens
            )
            return response.choices[0].message.content

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        result = wrapper.generate("Hello, how are you?", moral_value=0.9)

        assert result["accepted"] is True
        assert len(result["response"]) > 0
        assert openai_mock.chat.completions.create.called

    def test_openai_rate_limit_handling(self, openai_mock: Mock) -> None:
        """Test handling of OpenAI rate limit errors."""

        # Simulate rate limit error with eventual success
        call_count = 0
        def llm_generate_with_retry(prompt: str, max_tokens: int = 100) -> str:
            nonlocal call_count
            call_count += 1
            if call_count < 2:
                # LLMWrapper retries on RuntimeError
                raise RuntimeError("Rate limit exceeded")
            return "Success after retry"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_with_retry,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        # Should handle rate limit with retries
        result = wrapper.generate("Test prompt", moral_value=0.9)

        # Wrapper should retry and succeed
        assert call_count >= 2
        assert result["accepted"] is True

    def test_openai_timeout_handling(self) -> None:
        """Test handling of OpenAI timeout errors."""

        def llm_generate_timeout(prompt: str, max_tokens: int = 100) -> str:
            time.sleep(0.1)  # Simulate slow response
            raise TimeoutError("Request timed out")

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_timeout,
            embedding_fn=mock_embedding_fn,
            dim=384,
            wake_duration=10
        )

        # Wrapper catches timeout and returns error response
        result = wrapper.generate("Test prompt", moral_value=0.9)

        # Should return error response, not raise
        assert result["accepted"] is False
        assert "error" in result["note"].lower() or "failed" in result["note"].lower()

    def test_openai_invalid_api_key(self) -> None:
        """Test handling of invalid API key."""

        def llm_generate_auth_error(prompt: str, max_tokens: int = 100) -> str:
            raise Exception("Authentication failed: Invalid API key")

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_auth_error,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        # Wrapper catches auth errors and returns error response
        result = wrapper.generate("Test prompt", moral_value=0.9)

        assert result["accepted"] is False
        assert "error" in result["note"].lower() or "failed" in result["note"].lower()


# ============================================================================
# Local Model Tests (Ollama/llama.cpp)
# ============================================================================

class TestLocalModelIntegration:
    """Test integration with local models."""

    def test_ollama_mock_integration(self) -> None:
        """Test integration with Ollama-style local model."""

        def llm_generate_local(prompt: str, max_tokens: int = 100) -> str:
            """Mock Ollama-style response."""
            # Simulate local model response
            if "hello" in prompt.lower():
                return "Hello! I'm a local AI assistant."
            elif "python" in prompt.lower():
                return "Python is a programming language."
            return "I understand your question."

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_local,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        result = wrapper.generate("Hello, tell me about Python", moral_value=0.9)

        assert result["accepted"] is True
        assert len(result["response"]) > 0

    def test_local_model_latency(self) -> None:
        """Test latency characteristics of local model."""

        latencies = []

        def llm_generate_with_latency(prompt: str, max_tokens: int = 100) -> str:
            # Simulate variable local model latency
            delay = np.random.uniform(0.05, 0.15)
            time.sleep(delay)
            latencies.append(delay)
            return "Response from local model"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_with_latency,
            embedding_fn=mock_embedding_fn,
            dim=384,
            wake_duration=20
        )

        # Generate multiple requests
        for i in range(10):
            result = wrapper.generate(f"Test prompt {i}", moral_value=0.9)
            assert result["accepted"] is True

        # Verify latency distribution
        assert len(latencies) >= 10
        mean_latency = np.mean(latencies)
        assert 0.05 <= mean_latency <= 0.15

    def test_local_model_memory_efficiency(self) -> None:
        """Test that local model doesn't cause memory bloat."""

        def llm_generate_local(prompt: str, max_tokens: int = 100) -> str:
            return f"Response to: {prompt[:20]}"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_local,
            embedding_fn=mock_embedding_fn,
            capacity=100,
            wake_duration=50
        )

        # Process many requests
        for i in range(200):
            wrapper.generate(f"Test prompt {i}", moral_value=0.9)

        state = wrapper.get_state()

        # Memory should remain bounded
        assert state["qilm_stats"]["used"] <= 100
        assert state["qilm_stats"]["memory_mb"] < 100  # Should stay under 100MB


# ============================================================================
# Anthropic Claude API Tests
# ============================================================================

class TestAnthropicIntegration:
    """Test integration with Anthropic Claude API."""

    @pytest.fixture
    def claude_mock(self) -> Mock:
        """Create mock Anthropic client."""
        mock_client = Mock()
        mock_response = Mock()
        mock_response.content = [Mock(text="Claude response")]
        mock_client.messages.create.return_value = mock_response
        return mock_client

    def test_claude_basic_call(self, claude_mock: Mock) -> None:
        """Test basic Claude API call."""

        def llm_generate(prompt: str, max_tokens: int = 100) -> str:
            response = claude_mock.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        result = wrapper.generate("Hello Claude", moral_value=0.9)

        assert result["accepted"] is True
        assert len(result["response"]) > 0
        assert claude_mock.messages.create.called

    def test_claude_overloaded_handling(self, claude_mock: Mock) -> None:
        """Test handling of Claude API overload errors."""

        call_count = 0
        def llm_generate_with_overload(prompt: str, max_tokens: int = 100) -> str:
            nonlocal call_count
            call_count += 1
            if call_count < 2:
                # Use ConnectionError which triggers retry
                raise ConnectionError("Service temporarily unavailable")
            return "Success after overload"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_with_overload,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        result = wrapper.generate("Test prompt", moral_value=0.9)

        # Wrapper will retry and should succeed on second attempt
        assert call_count >= 2
        assert result["accepted"] is True

    def test_claude_streaming_mock(self) -> None:
        """Test mock streaming responses from Claude."""

        responses = []

        def llm_generate_streaming(prompt: str, max_tokens: int = 100) -> str:
            # Mock streaming by building response incrementally
            parts = ["This ", "is ", "a ", "streaming ", "response."]
            full_response = ""
            for part in parts:
                full_response += part
                responses.append(part)
            return full_response

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_streaming,
            embedding_fn=mock_embedding_fn,
            dim=384
        )

        result = wrapper.generate("Test streaming", moral_value=0.9)

        assert result["accepted"] is True
        assert len(responses) == 5


# ============================================================================
# Latency Distribution Tests
# ============================================================================

class TestLatencyDistribution:
    """Test and measure latency distributions."""

    def test_latency_measurement(self) -> None:
        """Measure actual latency distribution."""

        latencies = []

        def llm_generate_timed(prompt: str, max_tokens: int = 100) -> str:
            # Simulate realistic latency variation
            delay = np.random.lognormal(mean=np.log(0.1), sigma=0.3)
            time.sleep(min(delay, 0.5))  # Cap at 500ms for test speed
            return "Timed response"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_timed,
            embedding_fn=mock_embedding_fn,
            dim=384,
            wake_duration=50
        )

        # Collect latency data
        for i in range(30):
            start = time.time()
            result = wrapper.generate(f"Prompt {i}", moral_value=0.9)
            latency = time.time() - start

            if result["accepted"]:
                latencies.append(latency)

        # Calculate percentiles
        latencies_arr = np.array(latencies)
        p50 = np.percentile(latencies_arr, 50)
        p95 = np.percentile(latencies_arr, 95)
        p99 = np.percentile(latencies_arr, 99)

        print("\nLatency Distribution:")
        print(f"  P50: {p50*1000:.2f}ms")
        print(f"  P95: {p95*1000:.2f}ms")
        print(f"  P99: {p99*1000:.2f}ms")

        # Verify reasonable latency
        assert p50 < 1.0  # P50 should be under 1s
        assert p95 < 2.0  # P95 should be under 2s

    def test_latency_under_load(self) -> None:
        """Test latency behavior under simulated load."""

        latencies = []

        def llm_generate_variable(prompt: str, max_tokens: int = 100) -> str:
            # Simulate increasing latency under load
            current_load = len(latencies)
            base_delay = 0.05
            load_factor = 1 + (current_load / 100)  # Increases with load
            delay = base_delay * load_factor
            time.sleep(min(delay, 0.3))
            return "Response under load"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate_variable,
            embedding_fn=mock_embedding_fn,
            dim=384,
            wake_duration=100
        )

        # Process requests
        for i in range(50):
            start = time.time()
            result = wrapper.generate(f"Load test {i}", moral_value=0.9)
            latency = time.time() - start

            if result["accepted"]:
                latencies.append(latency)

        # Latency should increase with load
        early_latency = np.mean(latencies[:10])
        late_latency = np.mean(latencies[-10:])

        assert late_latency > early_latency


# ============================================================================
# Moral Filter with Toxic Inputs
# ============================================================================

class TestMoralFilterToxicity:
    """Test moral filter with real toxic inputs."""

    def get_toxic_samples(self) -> list[tuple[str, float]]:
        """Get simulated toxic input samples (text, moral_value)."""
        # Simulating HateSpeech dataset with various toxicity levels
        return [
            ("This is a normal message", 0.95),
            ("I disagree with that opinion", 0.85),
            ("That's a stupid idea", 0.45),
            ("You are an idiot", 0.25),
            ("I hate everyone", 0.15),
            ("Violent threat content", 0.05),
        ]

    def test_moral_filter_toxic_rejection(self) -> None:
        """Test that moral filter rejects highly toxic inputs."""

        def llm_generate(prompt: str, max_tokens: int = 100) -> str:
            return "Generated response"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate,
            embedding_fn=mock_embedding_fn,
            initial_moral_threshold=0.50,
            dim=384
        )

        toxic_samples = self.get_toxic_samples()

        results = []
        for text, moral_value in toxic_samples:
            result = wrapper.generate(text, moral_value=moral_value)
            results.append((text, moral_value, result["accepted"]))

        # High toxicity should be rejected
        highly_toxic = [r for r in results if r[1] < 0.3]
        assert all(not r[2] for r in highly_toxic), "Highly toxic inputs should be rejected"

        # Normal messages should be accepted
        normal = [r for r in results if r[1] > 0.8]
        assert any(r[2] for r in normal), "Normal messages should be accepted"

    def test_moral_filter_adaptation(self) -> None:
        """Test moral filter adaptation with varying inputs."""

        def llm_generate(prompt: str, max_tokens: int = 100) -> str:
            return "Response"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate,
            embedding_fn=mock_embedding_fn,
            initial_moral_threshold=0.50,
            dim=384,
            wake_duration=100
        )

        # Feed sequence of varying toxicity
        samples = self.get_toxic_samples() * 5  # Repeat to test adaptation

        initial_state = wrapper.get_state()
        initial_state["moral_threshold"]

        for text, moral_value in samples:
            wrapper.generate(text, moral_value=moral_value)

        final_state = wrapper.get_state()
        final_threshold = final_state["moral_threshold"]

        # Threshold should adapt (could increase or decrease based on input mix)
        # Just verify it's within valid range and has changed
        assert 0.30 <= final_threshold <= 0.90

    def test_moral_filter_statistics(self) -> None:
        """Test statistics collection for moral filtering."""

        def llm_generate(prompt: str, max_tokens: int = 100) -> str:
            return "Response"

        wrapper = LLMWrapper(
            llm_generate_fn=llm_generate,
            embedding_fn=mock_embedding_fn,
            initial_moral_threshold=0.50,
            dim=384,
            wake_duration=100
        )

        toxic_samples = self.get_toxic_samples()

        accepted_count = 0
        rejected_count = 0

        for text, moral_value in toxic_samples:
            result = wrapper.generate(text, moral_value=moral_value)
            if result["accepted"]:
                accepted_count += 1
            else:
                rejected_count += 1

        state = wrapper.get_state()

        # Verify statistics tracking
        assert state["accepted_count"] == accepted_count
        assert state["rejected_count"] == rejected_count
        assert state["step"] == len(toxic_samples)

        print("\nMoral Filter Statistics:")
        print(f"  Accepted: {accepted_count}")
        print(f"  Rejected: {rejected_count}")
        print(f"  Rejection Rate: {rejected_count/len(toxic_samples)*100:.1f}%")
        print(f"  Final Threshold: {state['moral_threshold']:.3f}")


# ============================================================================
# Main Test Runner
# ============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
