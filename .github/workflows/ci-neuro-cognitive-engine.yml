name: CI - Neuro Cognitive Engine

on:
  push:
    branches:
      - main
      - 'feature/*'
  pull_request:
    branches:
      - main
      - 'feature/*'

permissions:
  contents: read

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"
      
      - name: Run ruff linter
        run: |
          ruff check src tests
      
      - name: Run mypy type checker
        run: |
          mypy src/mlsdm

  security:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit
          pip install -r requirements.txt
      
      - name: Run pip-audit for vulnerability scanning
        run: |
          pip-audit --strict --progress-spinner=off
      
      - name: Report status
        if: failure()
        run: |
          echo "::error::Dependency vulnerabilities detected. Please update vulnerable packages."

  test:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    strategy:
      matrix:
        python-version: ['3.10', '3.11']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"
      
      - name: Run tests
        run: |
          pytest -q --ignore=tests/load
  
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"
      
      - name: Run E2E tests
        run: |
          pytest tests/e2e -v -m "not slow" --tb=short
  
  effectiveness-validation:
    name: Effectiveness Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"
      
      - name: Run effectiveness suite with SLO validation
        run: |
          python scripts/run_effectiveness_suite.py --validate-slo
      
      - name: Upload effectiveness snapshot
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: effectiveness-snapshot
          path: |
            reports/effectiveness_snapshot.json
            reports/EFFECTIVENESS_SNAPSHOT.md
          retention-days: 90
  
  benchmarks:
    name: Performance Benchmarks (SLO Gate)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"
      
      - name: Run benchmarks with SLO assertions
        id: benchmarks
        run: |
          # Run benchmarks and capture output
          echo "Running SLO-based performance benchmarks..."
          pytest benchmarks/test_neuro_engine_performance.py -v -s --tb=short \
            --junitxml=benchmark-results.xml 2>&1 | tee benchmark-output.txt
          
          # Extract P95 latencies from output for tracking
          python3 << 'EOF'
          import re
          import json
          import os
          
          with open("benchmark-output.txt", "r") as f:
              content = f.read()
          
          metrics = {
              "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "commit": os.environ.get("GITHUB_SHA", "unknown")[:8],
              "metrics": {}
          }
          
          # Extract P95 values
          p95_pattern = r"P95:\s+([0-9.]+)ms"
          matches = re.findall(p95_pattern, content)
          if matches:
              metrics["metrics"]["p95_latencies_ms"] = [float(m) for m in matches]
              metrics["metrics"]["max_p95_ms"] = max(float(m) for m in matches)
          
          # Check SLO compliance
          metrics["slo_compliant"] = "SLO met" in content
          
          with open("benchmark-metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          print(f"Extracted metrics: {json.dumps(metrics, indent=2)}")
          EOF
          
          # Check for SLO violations in output
          if grep -q "SLO met" benchmark-output.txt; then
            echo "âœ… All SLO targets met"
            echo "slo_status=passed" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ Some SLO targets may not have been verified"
            echo "slo_status=unknown" >> $GITHUB_OUTPUT
          fi
      
      - name: Check for performance regression (PERF-002)
        if: github.event_name == 'pull_request'
        run: |
          echo "## Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Get current P95 from metrics
          if [ -f benchmark-metrics.json ]; then
            CURRENT_P95=$(python3 -c "import json; d=json.load(open('benchmark-metrics.json')); print(d.get('metrics',{}).get('max_p95_ms', 'N/A'))")
            echo "Current max P95 latency: ${CURRENT_P95}ms" >> $GITHUB_STEP_SUMMARY
            
            # SLO threshold from SLO_SPEC.md
            SLO_THRESHOLD=500
            
            if [ "$CURRENT_P95" != "N/A" ]; then
              # Check if within SLO
              python3 << EOF
          import sys
          current = float("${CURRENT_P95}")
          threshold = ${SLO_THRESHOLD}
          regression_threshold = threshold * 0.8  # 80% of SLO = warning
          
          if current > threshold:
              print(f"âŒ REGRESSION: P95 {current}ms exceeds SLO of {threshold}ms")
              sys.exit(1)
          elif current > regression_threshold:
              print(f"âš ï¸ WARNING: P95 {current}ms approaching SLO of {threshold}ms")
          else:
              print(f"âœ… OK: P95 {current}ms well within SLO of {threshold}ms")
          EOF
            fi
          fi
      
      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### SLO Targets (from SLO_SPEC.md)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Target | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Pre-flight P95 latency | < 20ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| End-to-end P95 latency | < 500ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| Availability | â‰¥ 99.9% | ðŸ“Š Tracked in production |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark run completed at: $(date -u)" >> $GITHUB_STEP_SUMMARY
      
      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-py3.11
          path: |
            benchmark-results.xml
            benchmark-output.txt
            benchmark-metrics.json
          retention-days: 90
  
  neuro-engine-eval:
    name: Cognitive Safety Evaluation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    continue-on-error: true
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"
      
      - name: Run Sapolsky Validation Suite tests
        run: |
          pytest tests/eval/test_sapolsky_suite.py -v
      
      - name: Run Sapolsky Validation Suite evaluation
        run: |
          python examples/run_sapolsky_eval.py --output sapolsky_eval_results.json --verbose
        continue-on-error: true
      
      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sapolsky-eval-results
          path: |
            sapolsky_eval_results.json
          retention-days: 90
  
  # Gate job that requires all critical CI jobs to pass
  all-ci-passed:
    name: All CI Checks Passed
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [lint, security, test, e2e-tests, effectiveness-validation, benchmarks]
    steps:
      - name: All checks passed
        run: echo "All CI checks passed successfully!"
