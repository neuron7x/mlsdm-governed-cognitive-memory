name: CI - Neuro Cognitive Engine

on:
  push:
    branches:
      - main
      - 'feature/*'
  pull_request:
    branches:
      - main
      - 'feature/*'

permissions:
  contents: read

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"

      - name: Run lint (canonical)
        run: make lint

      - name: Run type check (canonical)
        run: make type

  ci-parity:
    name: CI Parity Check
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install parity dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml

      - name: Verify CI parity
        run: python scripts/verify_ci_parity.py

  security:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit
          pip install -r requirements.txt

      - name: Run pip-audit for vulnerability scanning
        run: |
          # WHY: Audit only requirements.txt dependencies to avoid false positives from system packages
          # Note: --format=json can be added for structured output if needed by reporting tools
          pip-audit --requirement requirements.txt --progress-spinner=off

      - name: Report status
        if: failure()
        run: |
          echo "::error::Dependency vulnerabilities detected. Please update vulnerable packages."

  test:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    strategy:
      matrix:
        python-version: ['3.10', '3.11']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run tests
        run: make test

  coverage:
    name: Code Coverage Gate
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run tests with coverage
        run: make coverage-gate

      - name: Verify coverage artifact
        run: test -f coverage.xml

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage.xml
          retention-days: 90

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run E2E tests
        run: make e2e

  effectiveness-validation:
    name: Effectiveness Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run effectiveness suite with SLO validation
        run: make effectiveness

      - name: Upload effectiveness snapshot
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: effectiveness-snapshot
          path: |
            reports/effectiveness_snapshot.json
            reports/EFFECTIVENESS_SNAPSHOT.md
          retention-days: 90

  benchmarks:
    name: Performance Benchmarks (SLO Gate)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run benchmarks with SLO assertions
        id: benchmarks
        run: make bench

      - name: Check for baseline drift
        if: always()
        run: |
          echo "## Baseline Drift Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run drift check (warning only, not strict)
          if [ -f benchmark-metrics.json ]; then
            python scripts/check_benchmark_drift.py benchmark-metrics.json --baseline benchmarks/baseline.json \
              | tee drift-check.txt

            # Add to summary
            cat drift-check.txt >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ benchmark-metrics.json not found, skipping drift check" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check for performance regression (PERF-002)
        if: github.event_name == 'pull_request'
        run: |
          echo "## Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Get current P95 from metrics
          if [ -f benchmark-metrics.json ]; then
            CURRENT_P95=$(python3 -c "import json; d=json.load(open('benchmark-metrics.json')); print(d.get('metrics',{}).get('max_p95_ms', 'N/A'))")
            echo "Current max P95 latency: ${CURRENT_P95}ms" >> $GITHUB_STEP_SUMMARY

            # SLO threshold from SLO_SPEC.md
            SLO_THRESHOLD=500

            if [ "$CURRENT_P95" != "N/A" ]; then
              # Check if within SLO
              python3 << EOF
          import sys
          current = float("${CURRENT_P95}")
          threshold = ${SLO_THRESHOLD}
          regression_threshold = threshold * 0.8  # 80% of SLO = warning

          if current > threshold:
              print(f"âŒ REGRESSION: P95 {current}ms exceeds SLO of {threshold}ms")
              sys.exit(1)
          elif current > regression_threshold:
              print(f"âš ï¸ WARNING: P95 {current}ms approaching SLO of {threshold}ms")
          else:
              print(f"âœ… OK: P95 {current}ms well within SLO of {threshold}ms")
          EOF
            fi
          fi

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### SLO Targets (from SLO_SPEC.md)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Target | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Pre-flight P95 latency | < 20ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| End-to-end P95 latency | < 500ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| Availability | â‰¥ 99.9% | ðŸ“Š Tracked in production |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark run completed at: $(date -u)" >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-py3.11
          path: |
            benchmark-results.xml
            benchmark-output.txt
            benchmark-metrics.json
          retention-days: 90

  neuro-engine-eval:
    name: Cognitive Safety Evaluation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    # INFORMATIONAL: Sapolsky evaluation suite is research/exploratory
    # This validates cognitive safety but is not a hard gate for CI
    # Failures here indicate areas for research/improvement, not blocking issues
    continue-on-error: true
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run Sapolsky Validation Suite tests
        run: |
          pytest tests/eval/test_sapolsky_suite.py -v

      - name: Run Sapolsky Validation Suite evaluation
        run: |
          python examples/run_sapolsky_eval.py --output sapolsky_eval_results.json --verbose
        # INFORMATIONAL: Evaluation script may fail for research/experimental features
        # Does not impact core functionality
        continue-on-error: true

      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: sapolsky-eval-results
          path: |
            sapolsky_eval_results.json
          retention-days: 90

  # Gate job that requires all critical CI jobs to pass
  # WHY: neuro-engine-eval is excluded because it has continue-on-error: true (non-blocking)
  all-ci-passed:
    name: All CI Checks Passed
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [ci-parity, lint, security, test, coverage, e2e-tests, effectiveness-validation, benchmarks]
    steps:
      - name: All checks passed
        run: echo "All CI checks passed successfully!"
