name: CI - Neuro Cognitive Engine

on:
  push:
    branches:
      - main
      - 'feature/*'
  pull_request:
    branches:
      - main
      - 'feature/*'

permissions:
  contents: read

jobs:
  lint:
    name: Lint and Type Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"

      - name: Run ruff linter
        run: |
          echo "Running ruff linter with detailed output..."
          ruff check src tests --output-format=github --show-fixes

      - name: Check for hidden Unicode characters
        run: |
          echo "Checking for hidden/bidirectional Unicode characters (Trojan Source protection)..."
          python scripts/check_no_bidi_unicode.py

      - name: Run mypy type checker
        run: make type

  security:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit
          pip install -r requirements.txt

      - name: Run pip-audit for vulnerability scanning
        run: |
          # WHY: Audit only requirements.txt dependencies to avoid false positives from system packages
          # Note: --format=json can be added for structured output if needed by reporting tools
          pip-audit --requirement requirements.txt --progress-spinner=off

      - name: Report status
        if: failure()
        run: |
          echo "::error::Dependency vulnerabilities detected. Please update vulnerable packages."

  test:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run tests
        run: |
          # Exclude benchmark tests from CI matrix - they are timing-sensitive and run
          # separately in the benchmarks job on Python 3.11 with fixed environment.
          # This prevents flaky failures from CI virtualization overhead variability.
          pytest -q --ignore=tests/load -m "not benchmark"

  coverage:
    name: Code Coverage Gate
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run tests with coverage
        run: |
          # WHY: Generate coverage report for src/mlsdm with staged threshold targets
          # Staged thresholds: 65% (legacy) -> 75% (current) -> 85% -> 90% (target)
          # Current coverage: ~74%. Branch coverage enabled via pyproject.toml
          # See TEST_STRATEGY.md for coverage model and rationale
          # NOTE: Exclude slow/benchmark tests to avoid flaky performance tests blocking coverage gate
          pytest --cov=src/mlsdm --cov-report=xml --cov-report=json --cov-report=term-missing \
            --cov-branch --cov-fail-under=75 --ignore=tests/load -m "not slow and not benchmark" -v

      - name: Validate Core Modules Coverage â‰¥95%
        run: |
          # WHY: Ensure core modules (core, memory, cognition) maintain â‰¥95% coverage
          # These are critical production modules that require higher coverage standards
          # NOTE: Uses coverage.json generated by the previous step (no re-run of tests)
          python scripts/validate_coverage_95.py

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: coverage-report
          path: |
            coverage.xml
            coverage.json
          retention-days: 90

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run E2E tests
        run: |
          pytest tests/e2e -v -m "not slow" --tb=short

  effectiveness-validation:
    name: Effectiveness Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run effectiveness suite with SLO validation
        run: |
          python scripts/run_effectiveness_suite.py --validate-slo

      - name: Upload effectiveness snapshot
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: effectiveness-snapshot
          path: |
            reports/effectiveness_snapshot.json
            reports/EFFECTIVENESS_SNAPSHOT.md
          retention-days: 90

  benchmarks:
    name: Performance Benchmarks (SLO Gate)
    runs-on: ubuntu-latest
    timeout-minutes: 10
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run benchmarks with SLO assertions
        id: benchmarks
        run: |
          # Run benchmarks and capture output
          # Includes both benchmarks/ directory AND tests/perf/ with @pytest.mark.benchmark
          # These are timing-sensitive tests that need stable, single-version environment
          echo "Running SLO-based performance benchmarks..."
          pytest benchmarks/test_neuro_engine_performance.py tests/perf/ -v -s --tb=short \
            -m "benchmark" --junitxml=benchmark-results.xml 2>&1 | tee benchmark-output.txt

          # Extract P95 latencies from output for tracking
          # WHY: Use Python datetime for accurate timestamps instead of shell command
          python3 << 'EOF'
          import re
          import json
          import os
          from datetime import datetime, timezone

          with open("benchmark-output.txt", "r") as f:
              content = f.read()

          metrics = {
              "timestamp": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
              "commit": os.environ.get("GITHUB_SHA", "unknown")[:8],
              "metrics": {}
          }

          # Extract P95 values
          p95_pattern = r"P95:\s+([0-9.]+)ms"
          matches = re.findall(p95_pattern, content)
          if matches:
              metrics["metrics"]["p95_latencies_ms"] = [float(m) for m in matches]
              metrics["metrics"]["max_p95_ms"] = max(float(m) for m in matches)

          # Check SLO compliance
          metrics["slo_compliant"] = "SLO met" in content

          with open("benchmark-metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)

          print(f"Extracted metrics: {json.dumps(metrics, indent=2)}")
          EOF

          # Check for SLO violations in output
          if grep -q "SLO met" benchmark-output.txt; then
            echo "âœ… All SLO targets met"
            echo "slo_status=passed" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ Some SLO targets may not have been verified"
            echo "slo_status=unknown" >> $GITHUB_OUTPUT
          fi

      - name: Check for baseline drift
        if: always()
        run: |
          echo "## Baseline Drift Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Run drift check (warning only, not strict)
          if [ -f benchmark-metrics.json ]; then
            python scripts/check_benchmark_drift.py benchmark-metrics.json --baseline benchmarks/baseline.json \
              | tee drift-check.txt

            # Add to summary
            cat drift-check.txt >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ benchmark-metrics.json not found, skipping drift check" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check for performance regression (PERF-002)
        if: github.event_name == 'pull_request'
        run: |
          echo "## Performance Regression Check" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Get current P95 from metrics
          if [ -f benchmark-metrics.json ]; then
            CURRENT_P95=$(python3 -c "import json; d=json.load(open('benchmark-metrics.json')); print(d.get('metrics',{}).get('max_p95_ms', 'N/A'))")
            echo "Current max P95 latency: ${CURRENT_P95}ms" >> $GITHUB_STEP_SUMMARY

            # SLO threshold from SLO_SPEC.md
            SLO_THRESHOLD=500

            if [ "$CURRENT_P95" != "N/A" ]; then
              # Check if within SLO
              python3 << EOF
          import sys
          current = float("${CURRENT_P95}")
          threshold = ${SLO_THRESHOLD}
          regression_threshold = threshold * 0.8  # 80% of SLO = warning

          if current > threshold:
              print(f"âŒ REGRESSION: P95 {current}ms exceeds SLO of {threshold}ms")
              sys.exit(1)
          elif current > regression_threshold:
              print(f"âš ï¸ WARNING: P95 {current}ms approaching SLO of {threshold}ms")
          else:
              print(f"âœ… OK: P95 {current}ms well within SLO of {threshold}ms")
          EOF
            fi
          fi

      - name: Generate benchmark summary
        if: always()
        run: |
          echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### SLO Targets (from SLO_SPEC.md)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Target | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Pre-flight P95 latency | < 20ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| End-to-end P95 latency | < 500ms | âœ… Verified in tests |" >> $GITHUB_STEP_SUMMARY
          echo "| Availability | â‰¥ 99.9% | ðŸ“Š Tracked in production |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark run completed at: $(date -u)" >> $GITHUB_STEP_SUMMARY

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results-py3.11
          path: |
            benchmark-results.xml
            benchmark-output.txt
            benchmark-metrics.json
          retention-days: 90

  neuro-engine-eval:
    name: Cognitive Safety Evaluation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    # INFORMATIONAL: Sapolsky evaluation suite is research/exploratory
    # This validates cognitive safety but is not a hard gate for CI
    # Failures here indicate areas for research/improvement, not blocking issues
    continue-on-error: true
    env:
      DISABLE_RATE_LIMIT: "1"
      LLM_BACKEND: "local_stub"

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[test]"

      - name: Run Sapolsky Validation Suite tests
        run: |
          pytest tests/eval/test_sapolsky_suite.py -v

      - name: Run Sapolsky Validation Suite evaluation
        run: |
          python examples/run_sapolsky_eval.py --output sapolsky_eval_results.json --verbose
        # INFORMATIONAL: Evaluation script may fail for research/experimental features
        # Does not impact core functionality
        continue-on-error: true

      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: sapolsky-eval-results
          path: |
            sapolsky_eval_results.json
          retention-days: 90

  # Gate job that requires all critical CI jobs to pass
  # WHY: neuro-engine-eval is excluded because it has continue-on-error: true (non-blocking)
  all-ci-passed:
    name: All CI Checks Passed
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [lint, security, test, coverage, e2e-tests, effectiveness-validation, benchmarks]
    steps:
      - name: All checks passed
        run: echo "All CI checks passed successfully!"
