name: Performance & Resilience Validation

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened, labeled]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering
    inputs:
      run_fast_resilience:
        description: 'Run Fast Resilience Tests'
        required: false
        type: boolean
        default: false
      run_performance_slo:
        description: 'Run Performance & SLO Validation'
        required: false
        type: boolean
        default: false
      run_comprehensive_resilience:
        description: 'Run Comprehensive Resilience Tests'
        required: false
        type: boolean
        default: false

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  changes:
    name: Detect relevant changes
    runs-on: ubuntu-latest
    outputs:
      relevant: ${{ steps.filter.outputs.relevant }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Detect code changes
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            relevant:
              - 'src/**'
              - 'tests/**'
              - 'examples/**'
              - 'config/**'
              - 'configs/**'
              - '.github/workflows/**'
              - 'pyproject.toml'
              - 'requirements*.txt'
              - 'uv.lock'
              - 'Dockerfile*'
              - 'Makefile'
              - 'scripts/**'

  # Fast resilience tests (run on all PRs)
  resilience-fast:
    name: Fast Resilience Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: changes

    # Run on: main branch, PRs with 'resilience' label, or nightly
    if: |
      github.event_name == 'workflow_dispatch' && inputs.run_fast_resilience ||
      github.event_name == 'schedule' ||
      github.ref == 'refs/heads/main' ||
      (github.event_name == 'pull_request' && needs.changes.outputs.relevant == 'true') ||
      (github.event_name == 'pull_request' && (
        contains(github.event.pull_request.labels.*.name, 'resilience') ||
        contains(github.event.pull_request.labels.*.name, 'perf')
      ))

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Restore Python cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-py3.11-${{ hashFiles('**/requirements.txt', '**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-py3.11-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"
          pip install pytest pytest-benchmark pytest-asyncio

      - name: Run fast resilience tests
        run: |
          python -m pytest tests/resilience/ \
            -v \
            -m "not slow" \
            --tb=short \
            --maxfail=5 \
            --junit-xml=resilience-fast-results.xml
        continue-on-error: false

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: resilience-fast-results
          path: resilience-fast-results.xml
          retention-days: 30

  # Performance/SLO tests (run on labeled PRs, main, and nightly)
  performance-slo:
    name: Performance & SLO Validation
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: changes

    if: |
      github.event_name == 'workflow_dispatch' && inputs.run_performance_slo ||
      github.event_name == 'schedule' ||
      github.ref == 'refs/heads/main' ||
      (github.event_name == 'pull_request' && needs.changes.outputs.relevant == 'true' && (
        contains(github.event.pull_request.labels.*.name, 'perf') ||
        contains(github.event.pull_request.labels.*.name, 'resilience')
      ))

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Restore Python cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-py3.11-${{ hashFiles('**/requirements.txt', '**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-py3.11-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"
          pip install pytest pytest-benchmark pytest-asyncio

      - name: Run SLO validation tests
        run: |
          python -m pytest tests/perf/ \
            -v \
            -m "benchmark and not slow" \
            --tb=short \
            --maxfail=3 \
            --junit-xml=perf-slo-results.xml
        env:
          DISABLE_RATE_LIMIT: "1"
        continue-on-error: false

      - name: Generate performance summary
        if: always()
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance and SLO validation completed." >> $GITHUB_STEP_SUMMARY
          echo "See artifacts for detailed results." >> $GITHUB_STEP_SUMMARY

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: perf-slo-results
          path: perf-slo-results.xml
          retention-days: 90

  # Comprehensive tests (slow, run only nightly or on-demand)
  resilience-comprehensive:
    name: Comprehensive Resilience Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && inputs.run_comprehensive_resilience)

    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Python 3.11
        uses: actions/setup-python@v6
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Restore Python cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-py3.11-${{ hashFiles('**/requirements.txt', '**/pyproject.toml', '**/uv.lock') }}
          restore-keys: |
            ${{ runner.os }}-py3.11-
            ${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e ".[dev]"
          pip install pytest pytest-benchmark pytest-asyncio

      - name: Run all resilience tests (including slow)
        run: |
          python -m pytest tests/resilience/ \
            -v \
            --tb=short \
            --junit-xml=resilience-comprehensive-results.xml
        # INFORMATIONAL: Comprehensive/slow tests run only nightly
        # These are for long-term monitoring and should not block CI
        continue-on-error: true

      - name: Run comprehensive performance tests
        run: |
          python -m pytest tests/perf/ \
            -v \
            -m "benchmark" \
            --tb=short \
            --junit-xml=perf-comprehensive-results.xml
        env:
          DISABLE_RATE_LIMIT: "1"
        # INFORMATIONAL: Comprehensive/slow tests run only nightly
        # These are for long-term monitoring and should not block CI
        continue-on-error: true

      - name: Upload comprehensive results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: resilience-comprehensive-results
          path: |
            resilience-comprehensive-results.xml
            perf-comprehensive-results.xml
          retention-days: 90

  # Gate job (required for merge)
  # Gate semantics:
  # - On main / labeled PRs / schedule: fast and perf jobs MUST succeed → gate fails if they fail.
  # - On unlabeled PRs where jobs are not scheduled: results = skipped → gate passes but logs a notice.
  perf-resilience-gate:
    name: Performance & Resilience Gate
    runs-on: ubuntu-latest
    needs: [changes, resilience-fast, performance-slo]
    if: always()

    steps:
      - name: Check job results
        env:
          RELEVANT: ${{ needs.changes.outputs.relevant }}
          IS_PR: ${{ github.event_name == 'pull_request' }}
          HAS_PERF_LABEL: ${{ contains(github.event.pull_request.labels.*.name, 'perf') || contains(github.event.pull_request.labels.*.name, 'resilience') }}
          RUN_DISPATCH_PERF: ${{ github.event_name == 'workflow_dispatch' && inputs.run_performance_slo }}
          IS_MAIN: ${{ github.ref == 'refs/heads/main' }}
          IS_SCHEDULE: ${{ github.event_name == 'schedule' }}
        run: |
          RES_FAST="${{ needs.resilience-fast.result }}"
          PERF_SLO="${{ needs.performance-slo.result }}"

          echo "Relevant changes detected: ${RELEVANT}"
          echo "Resilience-fast result: ${RES_FAST}"
          echo "Performance-slo result: ${PERF_SLO}"

          if [ "${RELEVANT}" = "true" ] && [ "${RES_FAST}" = "skipped" ]; then
            echo "::error::Resilience-fast was skipped despite relevant code changes."
            exit 1
          fi

          PERF_EXPECTED="false"
          if [ "${IS_MAIN}" = "true" ] || [ "${IS_SCHEDULE}" = "true" ] || [ "${RUN_DISPATCH_PERF}" = "true" ]; then
            PERF_EXPECTED="true"
          elif [ "${IS_PR}" = "true" ] && [ "${HAS_PERF_LABEL}" = "true" ] && [ "${RELEVANT}" = "true" ]; then
            PERF_EXPECTED="true"
          fi

          if [ "${PERF_EXPECTED}" = "true" ] && [ "${PERF_SLO}" = "skipped" ]; then
            echo "::error::Performance & SLO validation was expected but skipped."
            exit 1
          fi

          # If any scheduled job failed or was cancelled → hard failure
          if { [ "${RES_FAST}" = "failure" ] || [ "${RES_FAST}" = "cancelled" ]; } || \
             { [ "${PERF_SLO}" = "failure" ] || [ "${PERF_SLO}" = "cancelled" ]; }; then
            echo "::error::Performance or resilience tests failed (failure/cancelled)"
            exit 1
          fi

          # If both jobs were skipped → this PR simply didn't request perf/resilience tests
          if [ "${RES_FAST}" = "skipped" ] && [ "${PERF_SLO}" = "skipped" ]; then
            if [ "${RELEVANT}" = "true" ]; then
              echo "::error::Relevant changes detected but both jobs were skipped."
              exit 1
            fi
            echo "::notice::Performance/resilience test jobs were skipped because no relevant code changes were detected."
            exit 0
          fi

          # Otherwise (some combination of success/skipped) → all required checks passed
          echo "::notice::All required performance and resilience tests passed."
          exit 0

      - name: Report success
        run: |
          echo "## ✅ Performance & Resilience Validation Passed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "All required performance and resilience tests completed successfully." >> $GITHUB_STEP_SUMMARY
