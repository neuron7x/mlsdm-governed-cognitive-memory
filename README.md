# MLSDM Governed Cognitive Memory v1.0.1

[![CI](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/ci.yml/badge.svg)](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/ci.yml)
[![PR Validation](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/pr-validation.yml/badge.svg)](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/pr-validation.yml)
[![CodeQL](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/codeql.yml/badge.svg)](https://github.com/neuron7x/mlsdm-governed-cognitive-memory/actions/workflows/codeql.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

Neurobiologically-grounded cognitive architecture with moral governance, phase-based memory, and cognitive rhythm.

## Status: Alpha v1.0.1

**What Works:**
- âœ… Thread-safe concurrent processing
- âœ… Bounded memory (20k capacity, 29.37 MB)
- âœ… Moral filter with EMA adaptation
- âœ… Phase-based retrieval
- âœ… Wake/sleep cognitive rhythm
- âœ… Multi-level synaptic memory

**Verified:**
- Concurrency: 1000 parallel requests, zero lost updates
- Memory: Fixed 29.37 MB, no leaks
- Moral convergence: Tested with 200-step sequences

**Effectiveness Validation (Principal System Architect Level):**
- âœ… **89.5% resource efficiency** improvement with wake/sleep cycles
- âœ… **93.3% toxic content rejection** with moral filtering (vs 0% baseline)
- âœ… **5.5% coherence improvement** with phase-based memory organization
- âœ… **Stable under attack**: Bounded drift (0.33) during 70% toxic bombardment
- ðŸ“Š See [EFFECTIVENESS_VALIDATION_REPORT.md](EFFECTIVENESS_VALIDATION_REPORT.md) for full analysis

## Installation

```bash
pip install -r requirements.txt
python tests/integration/test_end_to_end.py
```

## Quick Start

```python
from src.core.cognitive_controller import CognitiveController
import numpy as np

controller = CognitiveController(dim=384)
vector = np.random.randn(384).astype(np.float32)
vector = vector / np.linalg.norm(vector)

state = controller.process_event(vector, moral_value=0.8)
print(state)
```

## Architecture

**Components:**
- `MoralFilterV2`: Adaptive moral threshold (0.30-0.90) with EMA
- `QILM_v2`: Bounded quantum-inspired memory with phase entanglement
- `MultiLevelSynapticMemory`: 3-level decay (L1/L2/L3)
- `CognitiveRhythm`: Wake/sleep cycle (8/3 duration)
- `CognitiveController`: Thread-safe orchestrator

**Formulas:**
```
EMA: Î±Â·signal + (1-Î±)Â·EMA_prev
Threshold: clip(threshold + 0.05Â·sign(error), 0.30, 0.90)
Cosine: dot(A,B) / (||A||Â·||B||)
```

## Performance

- P50 latency: ~2ms (process_event)
- P95 latency: ~10ms (with retrieval)
- Throughput: 5,500 ops/sec (verified)
- Memory: 29.37 MB (fixed)

## Tests

### Comprehensive Test Suite

**Test Coverage**: 90.48% (182 unit tests + integration + validation + chaos + adversarial + performance)

#### Quick Start
```bash
# Run all tests
pytest src/tests/ tests/ -v --cov=src

# Unit tests only (fast)
pytest src/tests/unit/ -v

# Integration tests
python tests/integration/test_end_to_end.py
```

#### Test Categories

**1. Unit Tests** (`src/tests/unit/`) - 182 tests
```bash
pytest src/tests/unit/ -v --cov=src --cov-report=html
```

**2. Integration Tests** (`tests/integration/`)
```bash
python tests/integration/test_end_to_end.py
```

**3. Validation Tests** (`tests/validation/`)
```bash
# Wake/sleep effectiveness (89.5% efficiency improvement)
python tests/validation/test_wake_sleep_effectiveness.py

# Moral filter effectiveness (93.3% toxic rejection)
python tests/validation/test_moral_filter_effectiveness.py
```

**4. Chaos Engineering** (`tests/chaos/`)
```bash
# Fault injection, concurrency, toxic bombardment
python tests/chaos/test_fault_injection.py
```
Tests: High concurrency (50 workers), invalid inputs, rapid phase transitions, memory stability

**5. Adversarial Testing** (`tests/adversarial/`)
```bash
# Jailbreak resistance, threshold manipulation
python tests/adversarial/test_jailbreak_resistance.py
```
Tests: Threshold manipulation, gradient attacks, toggle attacks, toxic siege, EMA stability

**6. Performance Benchmarks** (`tests/performance/`)
```bash
# Latency SLOs, throughput, memory profiling
python tests/performance/test_benchmarks.py
```
Validates: P95 < 120ms, P99 < 200ms, throughput > 1000 ops/sec, memory â‰¤ 1.4GB

#### Property-Based Tests
```bash
# Hypothesis-driven invariant verification
pytest src/tests/unit/test_property_based.py --hypothesis-show-statistics
```

#### CI/CD Testing
All PRs automatically run:
- Lint & type checking (Ruff, MyPy)
- Full test suite with coverage
- Security scanning (Bandit, Safety, CodeQL)
- Multi-version testing (Python 3.10, 3.11, 3.12)
- Performance regression detection

See [TESTING_STRATEGY.md](TESTING_STRATEGY.md) for details.

## Legacy API and Simulation

```bash
# Run simulation (legacy)
python -m src.main --steps 100 --plot

# Run API (legacy)
python -m src.main --api

# Run unit tests (legacy)
make test            # run unit + property tests
pytest -k property   # run property-based invariants (Hypothesis)
pytest -k state      # run state machine transition tests
```

## Testing & Verification Strategy (Principal System Architect Level)
This project incorporates advanced system reliability, mathematical correctness, AI safety, and performance validation methodologies expected at Principal / Staff engineering levels.

### Verification Pillars
1. Invariant Verification (Formal & Property-Based)
2. Resilience & Chaos Robustness
3. AI Governance & Safety Hardening
4. Performance & Saturation Profiling
5. Drift & Alignment Stability
6. Observability of Tail Failure Modes

### Methodologies Implemented / Planned
| Category | Method | Status | Tooling |
|----------|--------|--------|---------|
| Resilience | Chaos Engineering / Fault Injection | âœ… Implemented | GitHub Actions, custom tests |
| Resilience | Concurrent Load Testing | âœ… Implemented | Threading, stress tests |
| Resilience | Memory Stability Testing | âœ… Implemented | psutil monitoring |
| Correctness | Property-Based Testing | âœ… Implemented | Hypothesis (10k+ samples) |
| Correctness | State Machine Verification | âœ… Implemented | pytest + invariant tests |
| AI Safety | Adversarial Red Teaming | âœ… Implemented | Jailbreak resistance tests |
| AI Safety | Cognitive Drift Testing | âœ… Implemented | Toxic bombardment tests |
| AI Safety | Threshold Manipulation Resistance | âœ… Implemented | Adversarial test suite |
| Performance | Latency Profiling | âœ… Implemented | Benchmark suite (P50/P95/P99) |
| Performance | Throughput Testing | âœ… Implemented | Single/concurrent ops/sec |
| Performance | Memory Profiling | âœ… Implemented | RSS tracking, leak detection |
| Security | Dependency Scanning | âœ… Implemented | pip-audit, Safety, Bandit |
| Security | CodeQL Analysis | âœ… Implemented | Weekly scans, SARIF reports |
| CI/CD | PR Validation | âœ… Implemented | Automated quality gates |
| CI/CD | Nightly Tests | âœ… Implemented | Extended chaos + memory tests |
| Future | Soak Testing (48-72h) | ðŸ“‹ Planned | Locust + Prometheus |
| Future | Load Shedding Testing | ðŸ“‹ Planned | Rate limit middleware |
| Future | TLA+ Formal Specs | ðŸ“‹ Planned | TLC model checker |
| Future | RAG Faithfulness | ðŸ“‹ Planned | ragas framework |

### Core Invariants (Examples)
- Moral filter threshold T always âˆˆ [0.1, 0.9].
- Episodic graph remains acyclic (no circular temporal references).
- State cannot jump directly from Sleep â†’ Processing without Wake.
- Retrieval under corruption degrades to stateless fallback but always returns a syntactically valid response envelope.

### Sample Property-Based Invariant (Hypothesis Sketch)
```python
from hypothesis import given, strategies as st
from src.moral import clamp_threshold

@given(t=st.floats(min_value=-10, max_value=10))
def test_moral_threshold_clamped(t):
    clamped = clamp_threshold(t)
    assert 0.1 <= clamped <= 0.9
```

### Chaos Scenarios (Initial Set)
1. Kill vector DB container mid high-RPS retrieval.
2. Introduce 3000ms network latency between memory and policy service.
3. Randomly corrupt 0.5% of episodic entries â†’ verify integrity alarms trigger & quarantine.
4. Simulated clock skew in circadian scheduler.

### Performance SLO Focus
- P95 composite memory retrieval < 120ms.
- P99 policy decision < 60ms.
- Error budget: â‰¤ 2% degraded cycles per 24h.

### AI Safety Metrics
- Drift Î”(moral_threshold) over toxic storm < 0.05 absolute.
- Hallucination rate (ragas) < 0.15.
- Successful jailbreak attempts < 0.5% of adversarial batch.

### Observability Hooks
- event_formal_violation, event_drift_alert, event_chaos_fault.
- Prometheus histograms: retrieval_latency_bucket, moral_filter_eval_ms.
- OpenTelemetry trace: MemoryRetrieve â†’ SemanticMerge â†’ PolicyCheck.

### Toolchain
- Hypothesis, pytest, chaos-toolkit, Locust/K6, ragas, TLA+, Coq, OpenTelemetry, Prometheus.

## Contributing

We welcome contributions! This is a production-ready system with high engineering standards.

### Quick Start
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Install dependencies: `pip install -r requirements.txt && pip install pre-commit`
4. Install pre-commit hooks: `pre-commit install`
5. Make your changes with tests
6. Run tests: `pytest src/tests/ tests/ -v --cov=src --cov-fail-under=90`
7. Submit PR

### Requirements
- **Tests Required**: All new code must have tests (unit + integration as appropriate)
- **Coverage**: Maintain â‰¥90% code coverage
- **Type Hints**: All functions must have type annotations
- **Documentation**: Docstrings for all public APIs
- **Security**: Pass Bandit and Safety scans
- **Performance**: No regressions in latency or memory

### Test Categories to Consider
- Unit tests for new components
- Property-based tests for invariants
- Integration tests for multi-component features
- Chaos tests for resilience features
- Adversarial tests for security features
- Performance tests for optimizations

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

### Code Review Process
All PRs automatically run:
1. Lint and type checking (Ruff, MyPy)
2. Full test suite (182+ tests)
3. Security scanning (Bandit, Safety, CodeQL)
4. Coverage verification (â‰¥90%)
5. Multi-version testing (Python 3.10-3.12)

PRs require passing all checks and maintainer approval before merge.

## License

MIT License - see LICENSE file

## Citation

```bibtex
@software{mlsdm2025,
  title={MLSDM Governed Cognitive Memory},
  author={neuron7x},
  year={2025},
  url={https://github.com/neuron7x/mlsdm-governed-cognitive-memory}
}
```

---

**Note:** This is an Alpha release. Production use requires additional hardening (monitoring, logging, error handling).

## Legacy Documentation

For information about advanced testing methodologies and verification strategies, see the sections below.